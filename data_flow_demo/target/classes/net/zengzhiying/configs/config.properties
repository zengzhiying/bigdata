# kafka spout对应的zookeeper服务器地址 多个用逗号分隔
kafkaZookeeper = bigdata:2181
# kafka在Zookeeper中节点目录
kafkaZkDir = /kafka
# 配置Storm从Kafka获取数据的topic
topic = test
# kafka在Zookeeper中数据节点名称，随便设置一个有意义的字符串即可，不设置storm会自动生成
kafkaZkName = kafka-storm-topology
# 设置一个Spout task上面最多有多少个没有处理的Tuple (没有ack/failed)， 以防止Tuple队列爆掉
passinfo_topology_max_spout_pending = 500

# 供另外topology使用的的Topic名称
topic2 = topic2
# 入另外topology kafka的服务器列表，多个用逗号分隔
brokerList = bigdata1:9092,bigdata2:9092

# 设置Topology工作进程数
topology_workers_num = 2
# 设置Topology Acker线程的数目
topology_ackers_num = 2

# 设置kafka spout 并行度
kafka_spout_parallelism = 2

# 数据清洗Bolt并行度
data_parse_bolt_parallelism = 2
#Solr Bolt并行度
solr_bolt_parallelism = 2

# 本地模式运行topology的zookeeper配置 多个用逗号分隔，不用加端口
localZkServers = bigdata

# 批量大小(通过调整此参数优化入Solr的性能)
solr_batch_size = 200
# solr Collection名称
collection_name = test_collection
# solr zookeeper地址
solr_zk_hosts = bigdata:2181
# solr 在zookeeper上的节点
zk_solr_root = /solr